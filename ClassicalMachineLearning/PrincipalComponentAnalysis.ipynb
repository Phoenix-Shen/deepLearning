{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis\n",
    "\n",
    "- It is a statistical technique used to reduce the dimensionality of large datasets while retaining most of the information in the dataset. \n",
    "- PCA is used in various fields such as neuroscience, quantitative finance, image compression, facial recognition and many more.\n",
    "- Here we will implement PCA by scratch and have a full view of the method.\n",
    "- this blog is very good, I recommend reading it. <http://blog.codinglabs.org/articles/pca-tutorial.html>\n",
    "## Steps of PCA\n",
    "- Standardize the data\n",
    "- Calculate the covariance matrix\n",
    "- Calculate the eigenvector and eigenvalues of the covariance matrix\n",
    "- Sort the eigenvalues in descending order and choose $k$ igenvectors that correspond to the $k$ largest eigenvalues to form a new matrix.\n",
    "- Transform the original data into the new $k$ dimensional space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "\n",
    "class PCA():\n",
    "    def __init__(self,n_components:int):\n",
    "        \n",
    "        self.n_components = n_components\n",
    "    \n",
    "    def fit(self,X:ndarray,y:ndarray=None):\n",
    "        \n",
    "        # 1. norm data\n",
    "        X_std = (X-np.mean(X,axis=0))/X.std(axis=0)\n",
    "        \n",
    "        # 2. get the cov mat\n",
    "        cov_mat = np.cov(X_std.T)\n",
    "        # also we can use this line\n",
    "        # X_std.T@X_std/len(X_std)\n",
    "        \n",
    "        # 3. get the eigenvalues eigenvectors of the cov mat\n",
    "        eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n",
    "        \n",
    "        # 4. sort the eigenvalues in decreasing order and select top-k eigenvectors according to the eigenvalues\n",
    "        eig_pairs = [(np.abs(eig_vals[i]),eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "        eig_pairs.sort(key=lambda x: x[0], reverse=True)\n",
    "        matrix_w = np.hstack([(eig_pairs[i][1].reshape(len(eig_vals), 1)) for i in range(self.n_components)])\n",
    "        \n",
    "        # 5. store the matrix\n",
    "        self.matrix_w = matrix_w\n",
    "        self.X_std = X_std\n",
    "        self.eig_pairs = eig_pairs\n",
    "        \n",
    "    def fit_transform(self,X:ndarray,y:ndarray=None):\n",
    "        self.fit(X,y)\n",
    "        return self.X_std.dot(self.matrix_w)\n",
    "\n",
    "    def predict(self,X:ndarray,y:ndarray=None):\n",
    "        X_std = (X-np.mean(X,axis=0))/np.std(X,axis=0)\n",
    "        return X_std.dot(self.matrix_w)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data and test the PCA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()\n",
    "\n",
    "X = data.data\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca= PCA(2)\n",
    "X_pca = pca.fit_transform(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply a linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9733333333333334"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "cls = LogisticRegression(max_iter=500)\n",
    "\n",
    "cls.fit(X,y)\n",
    "cls.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9333333333333333"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls = LogisticRegression()\n",
    "\n",
    "cls.fit(X_pca,y)\n",
    "cls.score(X_pca,y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.9380850501999936,\n",
       " 0.920164904162487,\n",
       " 0.14774182104494776,\n",
       " 0.02085386217646217]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigen_values = [pca.eig_pairs[i][0] for i in range(len(pca.eig_pairs))]\n",
    "eigen_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAc50lEQVR4nO3df2xV53348c9tAMMCdgQNxhYmGLUlCWkyZKLhLpBkLGYQRWmF1Ki/IFUyzRuFJh5qY6i0JpvkSKOdSxvw0CBRmoSiyUmXFpriPzAkCtlmZrpuIyjVCEbULqPd7IRvdYHkfv9IY9XFBl/jy4NvXi/p/HHPPefc5+gB+a1zz703k8vlcgEAkMiHUg8AAPhgEyMAQFJiBABISowAAEmJEQAgKTECACQlRgCApMQIAJDUuNQDGI533303fv7zn8eUKVMik8mkHg4AMAy5XC7eeuutqKysjA99aOjrH2MiRn7+859HVVVV6mEAACNw/PjxmDlz5pDPj4kYmTJlSkS8dzKlpaWJRwMADEdfX19UVVX1/x0fypiIkfffmiktLRUjADDGXOwWCzewAgBJiREAICkxAgAklVeMbNmyJW6++eb+ezdqa2vjRz/60QX32bdvX9TU1MTEiRNjzpw50dLSckkDBgCKS14xMnPmzHj88cejo6MjOjo64o/+6I/i3nvvjf/8z/8cdPujR4/G8uXLY9GiRdHZ2Rnr16+PtWvXRmtr66gMHgAY+zK5XC53KQeYOnVq/O3f/m088MAD5z331a9+NV588cU4fPhw/7r6+vr4yU9+EgcOHBj2a/T19UVZWVn09vb6NA0AjBHD/fs94ntG3nnnnfje974Xp0+fjtra2kG3OXDgQNTV1Q1Yt3Tp0ujo6IizZ88OeexsNht9fX0DFgCgOOUdIz/96U9j8uTJUVJSEvX19fHCCy/EjTfeOOi2PT09UV5ePmBdeXl5nDt3Lk6dOjXkazQ1NUVZWVn/4ttXAaB45R0jc+fOjUOHDsVrr70Wf/7nfx6rVq2K//qv/xpy+9/9opP33xW60BegNDY2Rm9vb/9y/PjxfIcJAIwReX8D64QJE+IjH/lIREQsWLAg/vVf/zW+9a1vxd///d+ft+2MGTOip6dnwLqTJ0/GuHHjYtq0aUO+RklJSZSUlOQ7NABgDLrk7xnJ5XKRzWYHfa62tjba2toGrNuzZ08sWLAgxo8ff6kvDQAUgbxiZP369fHyyy/Hm2++GT/96U9jw4YN0d7eHp/73Oci4r23V1auXNm/fX19fRw7diwaGhri8OHDsX379ti2bVusW7dudM8CABiz8nqb5he/+EV84QtfiO7u7igrK4ubb745XnrppbjrrrsiIqK7uzu6urr6t6+uro7du3fHww8/HE888URUVlbGpk2bYsWKFaN7FgDAmHXJ3zNyOfieEQAYe4b79zvvG1iLzexHdqUewgfWm4/fnXoIAFwB/FAeAJCUGAEAkhIjAEBSYgQASEqMAABJiREAICkxAgAkJUYAgKTECACQlBgBAJISIwBAUmIEAEhKjAAASYkRACApMQIAJCVGAICkxAgAkJQYAQCSEiMAQFJiBABISowAAEmJEQAgKTECACQlRgCApMQIAJCUGAEAkhIjAEBSYgQASEqMAABJiREAICkxAgAkJUYAgKTECACQlBgBAJISIwBAUmIEAEhKjAAASYkRACApMQIAJCVGAICkxAgAkJQYAQCSEiMAQFJiBABISowAAEmJEQAgKTECACQlRgCApPKKkaamprj11ltjypQpMX369PjkJz8ZR44cueA+7e3tkclkzltef/31Sxo4AFAc8oqRffv2xerVq+O1116Ltra2OHfuXNTV1cXp06cvuu+RI0eiu7u7f/noRz864kEDAMVjXD4bv/TSSwMeP/nkkzF9+vQ4ePBgLF68+IL7Tp8+Pa655pq8BwgAFLdLumekt7c3IiKmTp160W3nz58fFRUVsWTJkti7d+8Ft81ms9HX1zdgAQCK04hjJJfLRUNDQ9x2221x0003DbldRUVFbN26NVpbW+P555+PuXPnxpIlS2L//v1D7tPU1BRlZWX9S1VV1UiHCQBc4TK5XC43kh1Xr14du3btildeeSVmzpyZ17733HNPZDKZePHFFwd9PpvNRjab7X/c19cXVVVV0dvbG6WlpSMZ7pBmP7JrVI/H8L35+N2phwBAAfX19UVZWdlF/36P6MrImjVr4sUXX4y9e/fmHSIREQsXLow33nhjyOdLSkqitLR0wAIAFKe8bmDN5XKxZs2aeOGFF6K9vT2qq6tH9KKdnZ1RUVExon0BgOKSV4ysXr06nnvuufinf/qnmDJlSvT09ERERFlZWUyaNCkiIhobG+PEiRPx9NNPR0REc3NzzJ49O+bNmxdnzpyJZ555JlpbW6O1tXWUTwUAGIvyipEtW7ZERMQdd9wxYP2TTz4Z999/f0REdHd3R1dXV/9zZ86ciXXr1sWJEydi0qRJMW/evNi1a1csX7780kYOABSFEd/AejkN9waYkXADazpuYAUobgW9gRUAYLSIEQAgKTECACQlRgCApMQIAJCUGAEAkhIjAEBSYgQASEqMAABJiREAICkxAgAkJUYAgKTECACQlBgBAJISIwBAUmIEAEhKjAAASYkRACApMQIAJCVGAICkxAgAkJQYAQCSEiMAQFJiBABISowAAEmJEQAgKTECACQlRgCApMQIAJCUGAEAkhIjAEBSYgQASEqMAABJiREAICkxAgAkJUYAgKTECACQlBgBAJISIwBAUmIEAEhKjAAASYkRACApMQIAJCVGAICkxAgAkJQYAQCSEiMAQFJiBABIKq8YaWpqiltvvTWmTJkS06dPj09+8pNx5MiRi+63b9++qKmpiYkTJ8acOXOipaVlxAMGAIpLXjGyb9++WL16dbz22mvR1tYW586di7q6ujh9+vSQ+xw9ejSWL18eixYtis7Ozli/fn2sXbs2WltbL3nwAMDYNy6fjV966aUBj5988smYPn16HDx4MBYvXjzoPi0tLTFr1qxobm6OiIgbbrghOjo6YuPGjbFixYqRjRoAKBqXdM9Ib29vRERMnTp1yG0OHDgQdXV1A9YtXbo0Ojo64uzZs5fy8gBAEcjryshvy+Vy0dDQELfddlvcdNNNQ27X09MT5eXlA9aVl5fHuXPn4tSpU1FRUXHePtlsNrLZbP/jvr6+kQ4TALjCjfjKyJe+9KX493//99ixY8dFt81kMgMe53K5Qde/r6mpKcrKyvqXqqqqkQ4TALjCjShG1qxZEy+++GLs3bs3Zs6cecFtZ8yYET09PQPWnTx5MsaNGxfTpk0bdJ/Gxsbo7e3tX44fPz6SYQIAY0Beb9PkcrlYs2ZNvPDCC9He3h7V1dUX3ae2tjZ+8IMfDFi3Z8+eWLBgQYwfP37QfUpKSqKkpCSfoQEAY1ReV0ZWr14dzzzzTDz33HMxZcqU6OnpiZ6envj1r3/dv01jY2OsXLmy/3F9fX0cO3YsGhoa4vDhw7F9+/bYtm1brFu3bvTOAgAYs/KKkS1btkRvb2/ccccdUVFR0b/s3Lmzf5vu7u7o6urqf1xdXR27d++O9vb2+P3f//3467/+69i0aZOP9QIAETGCt2ku5qmnnjpv3e233x7/9m//ls9LAQAfEH6bBgBISowAAEmJEQAgKTECACQlRgCApMQIAJCUGAEAkhIjAEBSYgQASEqMAABJiREAICkxAgAkJUYAgKTECACQlBgBAJISIwBAUmIEAEhKjAAASYkRACApMQIAJCVGAICkxAgAkJQYAQCSEiMAQFJiBABISowAAEmJEQAgKTECACQlRgCApMQIAJCUGAEAkhIjAEBSYgQASEqMAABJiREAICkxAgAkJUYAgKTECACQlBgBAJISIwBAUmIEAEhKjAAASYkRACApMQIAJCVGAICkxAgAkJQYAQCSEiMAQFJiBABIKu8Y2b9/f9xzzz1RWVkZmUwmvv/9719w+/b29shkMuctr7/++kjHDAAUkXH57nD69Om45ZZb4otf/GKsWLFi2PsdOXIkSktL+x9fe+21+b40AFCE8o6RZcuWxbJly/J+oenTp8c111yT934AQHG7bPeMzJ8/PyoqKmLJkiWxd+/ey/WyAMAVLu8rI/mqqKiIrVu3Rk1NTWSz2fjud78bS5Ysifb29li8ePGg+2Sz2chms/2P+/r6Cj1MACCRgsfI3LlzY+7cuf2Pa2tr4/jx47Fx48YhY6SpqSkeffTRQg8NALgCJPlo78KFC+ONN94Y8vnGxsbo7e3tX44fP34ZRwcAXE4FvzIymM7OzqioqBjy+ZKSkigpKbmMIwIAUsk7Rt5+++342c9+1v/46NGjcejQoZg6dWrMmjUrGhsb48SJE/H0009HRERzc3PMnj075s2bF2fOnIlnnnkmWltbo7W1dfTOAgAYs/KOkY6Ojrjzzjv7Hzc0NERExKpVq+Kpp56K7u7u6Orq6n/+zJkzsW7dujhx4kRMmjQp5s2bF7t27Yrly5ePwvABgLEuk8vlcqkHcTF9fX1RVlYWvb29A744bTTMfmTXqB6P4Xvz8btTDwGAAhru32+/TQMAJCVGAICkxAgAkJQYAQCSEiMAQFJiBABISowAAEmJEQAgKTECACQlRgCApMQIAJCUGAEAkhIjAEBSYgQASEqMAABJiREAICkxAgAkJUYAgKTECACQlBgBAJISIwBAUmIEAEhKjAAASYkRACApMQIAJCVGAICkxAgAkJQYAQCSEiMAQFJiBABISowAAEmJEQAgKTECACQlRgCApMQIAJCUGAEAkhIjAEBSYgQASEqMAABJiREAICkxAgAkJUYAgKTECACQlBgBAJISIwBAUmIEAEhKjAAASYkRACApMQIAJJV3jOzfvz/uueeeqKysjEwmE9///vcvus++ffuipqYmJk6cGHPmzImWlpaRjBUAKEJ5x8jp06fjlltuie985zvD2v7o0aOxfPnyWLRoUXR2dsb69etj7dq10dramvdgAYDiMy7fHZYtWxbLli0b9vYtLS0xa9asaG5ujoiIG264ITo6OmLjxo2xYsWKfF8eACgyBb9n5MCBA1FXVzdg3dKlS6OjoyPOnj076D7ZbDb6+voGLABAcSp4jPT09ER5efmAdeXl5XHu3Lk4derUoPs0NTVFWVlZ/1JVVVXoYQIAiVyWT9NkMpkBj3O53KDr39fY2Bi9vb39y/Hjxws+RgAgjbzvGcnXjBkzoqenZ8C6kydPxrhx42LatGmD7lNSUhIlJSWFHhoAcAUo+JWR2traaGtrG7Buz549sWDBghg/fnyhXx4AuMLlHSNvv/12HDp0KA4dOhQR731099ChQ9HV1RUR773FsnLlyv7t6+vr49ixY9HQ0BCHDx+O7du3x7Zt22LdunWjcwYAwJiW99s0HR0dceedd/Y/bmhoiIiIVatWxVNPPRXd3d39YRIRUV1dHbt3746HH344nnjiiaisrIxNmzb5WC8AEBERmdz7d5Newfr6+qKsrCx6e3ujtLR0VI89+5Fdo3o8hu/Nx+9OPQQACmi4f7/9Ng0AkJQYAQCSEiMAQFIF/54RSMX9QOm4HwjIhysjAEBSYgQASEqMAABJiREAICkxAgAkJUYAgKTECACQlBgBAJISIwBAUmIEAEhKjAAASYkRACApMQIAJCVGAICkxAgAkJQYAQCSEiMAQFJiBABISowAAEmJEQAgKTECACQlRgCApMQIAJCUGAEAkhIjAEBSYgQASEqMAABJiREAICkxAgAkJUYAgKTECACQlBgBAJISIwBAUmIEAEhKjAAASYkRACApMQIAJCVGAICkxAgAkJQYAQCSEiMAQFJiBABISowAAEmJEQAgKTECACQ1ohjZvHlzVFdXx8SJE6OmpiZefvnlIbdtb2+PTCZz3vL666+PeNAAQPHIO0Z27twZDz30UGzYsCE6Oztj0aJFsWzZsujq6rrgfkeOHInu7u7+5aMf/eiIBw0AFI+8Y+Sb3/xmPPDAA/Hggw/GDTfcEM3NzVFVVRVbtmy54H7Tp0+PGTNm9C9XXXXViAcNABSPvGLkzJkzcfDgwairqxuwvq6uLl599dUL7jt//vyoqKiIJUuWxN69ey+4bTabjb6+vgELAFCc8oqRU6dOxTvvvBPl5eUD1peXl0dPT8+g+1RUVMTWrVujtbU1nn/++Zg7d24sWbIk9u/fP+TrNDU1RVlZWf9SVVWVzzABgDFk3Eh2ymQyAx7ncrnz1r1v7ty5MXfu3P7HtbW1cfz48di4cWMsXrx40H0aGxujoaGh/3FfX58gAYAildeVkQ9/+MNx1VVXnXcV5OTJk+ddLbmQhQsXxhtvvDHk8yUlJVFaWjpgAQCKU14xMmHChKipqYm2trYB69va2uITn/jEsI/T2dkZFRUV+bw0AFCk8n6bpqGhIb7whS/EggULora2NrZu3RpdXV1RX18fEe+9xXLixIl4+umnIyKiubk5Zs+eHfPmzYszZ87EM888E62trdHa2jq6ZwIAjEl5x8h9990Xv/zlL+Oxxx6L7u7uuOmmm2L37t1x3XXXRUREd3f3gO8cOXPmTKxbty5OnDgRkyZNinnz5sWuXbti+fLlo3cWAMCYlcnlcrnUg7iYvr6+KCsri97e3lG/f2T2I7tG9XgM35uP313Q45vbdAo9t8DYMNy/336bBgBISowAAEmJEQAgKTECACQlRgCApMQIAJCUGAEAkhIjAEBSYgQASEqMAABJiREAICkxAgAkJUYAgKTECACQlBgBAJISIwBAUmIEAEhKjAAASYkRACApMQIAJCVGAICkxAgAkJQYAQCSEiMAQFJiBABISowAAEmJEQAgKTECACQlRgCApMQIAJCUGAEAkhIjAEBSYgQASEqMAABJiREAICkxAgAkNS71AADyNfuRXamH8IH15uN3px4CRciVEQAgKTECACQlRgCApMQIAJCUGAEAkhIjAEBSYgQASEqMAABJiREAICkxAgAkJUYAgKTECACQ1IhiZPPmzVFdXR0TJ06MmpqaePnlly+4/b59+6KmpiYmTpwYc+bMiZaWlhENFgAoPnnHyM6dO+Ohhx6KDRs2RGdnZyxatCiWLVsWXV1dg25/9OjRWL58eSxatCg6Oztj/fr1sXbt2mhtbb3kwQMAY9+4fHf45je/GQ888EA8+OCDERHR3NwcP/7xj2PLli3R1NR03vYtLS0xa9asaG5ujoiIG264ITo6OmLjxo2xYsWKSxs9AEVj9iO7Ug/hA+vNx+9O+vp5xciZM2fi4MGD8cgjjwxYX1dXF6+++uqg+xw4cCDq6uoGrFu6dGls27Ytzp49G+PHjz9vn2w2G9lstv9xb29vRET09fXlM9xheTf7/0b9mAxPIebzt5nbdMxt8Srk3JrXdAo1r+8fN5fLXXC7vGLk1KlT8c4770R5efmA9eXl5dHT0zPoPj09PYNuf+7cuTh16lRUVFSct09TU1M8+uij562vqqrKZ7hc4cqaU4+AQjG3xcvcFqdCz+tbb70VZWVlQz6f99s0ERGZTGbA41wud966i20/2Pr3NTY2RkNDQ//jd999N371q1/FtGnTLvg6HzR9fX1RVVUVx48fj9LS0tTDYRSZ2+JkXouXuR1cLpeLt956KyorKy+4XV4x8uEPfziuuuqq866CnDx58ryrH++bMWPGoNuPGzcupk2bNug+JSUlUVJSMmDdNddck89QP1BKS0v94y9S5rY4mdfiZW7Pd6ErIu/L69M0EyZMiJqammhraxuwvq2tLT7xiU8Muk9tbe152+/ZsycWLFgw6P0iAMAHS94f7W1oaIh/+Id/iO3bt8fhw4fj4Ycfjq6urqivr4+I995iWblyZf/29fX1cezYsWhoaIjDhw/H9u3bY9u2bbFu3brROwsAYMzK+56R++67L375y1/GY489Ft3d3XHTTTfF7t2747rrrouIiO7u7gHfOVJdXR27d++Ohx9+OJ544omorKyMTZs2+VjvKCgpKYm/+qu/Ou8tLcY+c1uczGvxMreXJpO72OdtAAAKyG/TAABJiREAICkxAgAkJUYAgKTESGL3339/ZDKZyGQyMX78+JgzZ06sW7cuTp8+3b9Na2tr3HHHHVFWVhaTJ0+Om2++OR577LH41a9+NazX2LdvX9TU1MTEiRNjzpw50dLSUqjT4bcUem67u7vjs5/9bMydOzc+9KEPxUMPPVTAs+F9hZ7X559/Pu6666649tpro7S0NGpra+PHP/5xIU+J3yj03L7yyivxh3/4hzFt2rSYNGlSXH/99fF3f/d3hTylMUOMXAH+5E/+JLq7u+O///u/42/+5m9i8+bN/d/DsmHDhrjvvvvi1ltvjR/96EfxH//xH/GNb3wjfvKTn8R3v/vdix776NGjsXz58li0aFF0dnbG+vXrY+3atdHa2lro0yIKO7fZbDauvfba2LBhQ9xyyy2FPhV+SyHndf/+/XHXXXfF7t274+DBg3HnnXfGPffcE52dnYU+LaKwc3v11VfHl770pdi/f38cPnw4vva1r8XXvva12Lp1a6FP68qXI6lVq1bl7r333gHrHnzwwdyMGTNy//zP/5yLiFxzc/Og+/7v//7vRY//la98JXf99dcPWPdnf/ZnuYULF450yAxToef2t91+++25L3/5yyMbKHm5nPP6vhtvvDH36KOPjmhfhi/F3H7qU5/Kff7znx/RvsXElZEr0KRJk+Ls2bPx7LPPxuTJk+Mv/uIvBt1uOL/Xc+DAgairqxuwbunSpdHR0RFnz54djeGSh9GcW64chZzXd999N956662YOnXqJY6SkSjk3HZ2dsarr74at99++yWOcuwTI1eYf/mXf4nnnnsulixZEm+88UbMmTPnkn7Dp6en57wfMSwvL49z587FqVOnLnW45GG055YrQ6Hn9Rvf+EacPn06Pv3pT4/aMRmeQs3tzJkzo6SkJBYsWBCrV6+OBx98cBRGO7aJkSvAD3/4w5g8eXJMnDgxamtrY/HixfHtb387crlcZDKZSz7+7x4j95sv3R2NY3NhhZ5b0rhc87pjx474+te/Hjt37ozp06eP2nEZ2uWY25dffjk6OjqipaUlmpubY8eOHaNy3LEs79+mYfTdeeedsWXLlhg/fnxUVlb2l/fHPvaxeOWVV+Ls2bMjrvEZM2ZET0/PgHUnT56McePGxbRp0y557FxYIeeWdC7HvO7cuTMeeOCB+Md//Mf44z/+49EYNsNwOea2uro6IiI+/vGPxy9+8Yv4+te/Hp/5zGcueexjmSsjV4Crr746PvKRj8R111034B/5Zz/72Xj77bdj8+bNg+73f//3fxc9dm1tbbS1tQ1Yt2fPnliwYIE/gpdBIeeWdAo9rzt27Ij7778/nnvuubj77rtHY8gM0+X+P5vL5SKbzY5o32LiysgV7A/+4A/iK1/5SvzlX/5lnDhxIj71qU9FZWVl/OxnP4uWlpa47bbb4stf/vIFj1FfXx/f+c53oqGhIf70T/80Dhw4ENu2bXNZMLHRmNuIiEOHDkVExNtvvx3/8z//E4cOHYoJEybEjTfeWOAzYDCjMa87duyIlStXxre+9a1YuHBh/5XNSZMmRVlZ2eU4DQYxGnP7xBNPxKxZs+L666+PiPe+d2Tjxo2xZs2ay3EKV7aUH+Vh8I+S/a6dO3fmFi9enJsyZUru6quvzt188825xx57bNgfJWtvb8/Nnz8/N2HChNzs2bNzW7ZsufSBc1GXY24j4rzluuuuu+SxM7RCz+vtt98+6LyuWrVqVMbP0Ao9t5s2bcrNmzcv93u/93u50tLS3Pz583ObN2/OvfPOO6NzAmNYJpf7zd2MAAAJuGcEAEhKjIxx8+bNi8mTJw+6PPvss6mHxyUwt8XJvBYvczty3qYZ444dOzbkN6mWl5fHlClTLvOIGC3mtjiZ1+JlbkdOjAAASXmbBgBISowAAEmJEQAgKTECACQlRgCApMQIAJCUGAEAkhIjAEBS/x/KdWZnwoMd8gAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.bar([\"PC_{}\".format(i) for i in range(len(eigen_values))],eigen_values)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find that at the sacrifice of a little accuracy, we can reduce the dimension of the data from 4 to 2."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mathmatical Knowledge Behind PCA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Unsupervised\n",
    "- Continuous\n",
    "- Dimensionality Reduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary\n",
    "\n",
    "- Mean\n",
    "  $$\n",
    "  \\bar{x} = \\frac{1}{n} \\sum_{i=1}^N x_i\n",
    "  $$\n",
    "\n",
    "- Variance\n",
    "  $$\n",
    "  S^2 = \\frac{1}{n-1} \\sum_{i=1}^N (x_i-\\bar{x})^2\n",
    "  $$\n",
    "\n",
    "- Covariance\n",
    "  $$\n",
    "  Cov(X,Y) = \\mathbb{E}[(X-\\mathbb{E}(X))(Y-\\mathbb{E}(Y))]\n",
    "  = \\frac{1}{n-1} \\sum_{i=1}^N (x_i-\\bar{x})(y_i-\\bar{y})\n",
    "  $$\n",
    "\n",
    "variance is the special case of covariance\n",
    "\n",
    "## What does each step mean?\n",
    "\n",
    "1. Norm the data\n",
    "\n",
    "    ```python\n",
    "    X_std = (X-np.mean(X,axis=0))/np.std(X,axis=0)\n",
    "    ```\n",
    "\n",
    "    Assume the data has $N$ samples and each sample has $d$ features.\n",
    "\n",
    "    $$\n",
    "    \\mathbf{X} = [X_1,X_2,\\dots,X_N]\\\\\n",
    "    X_i = [f_{i1},f_{i2},\\dots,f_{id}]\n",
    "    $$\n",
    "\n",
    "    We normalize the data using the following equations, it will be used in the next step!\n",
    "    $$\n",
    "    \\tilde{X}_i =[\\tilde{f}_{i1},\\tilde{f}_{i2},\\dots,\\tilde{f}_{id}] \\\\\n",
    "    \\tilde{f}_{ij} = \\frac{f_{ij} - \\bar{f}_{ij}}{\\sqrt{\\frac{1}{N}\\sum_{k=1}^N (f_{kj}-\\bar{f}_{kj})^2}}\\\\\n",
    "    \\bar{f}_{ij} = f_{ij} - \\frac{1}{N}\\sum_{k=1}^N f_{kj}\n",
    "    $$\n",
    "\n",
    "    substract mean and divide by the standard deviation.\n",
    "\n",
    "    The reason why we divide by the standard deviation is to ensure that all features are on a similar scale. This is important because PCA is sensitive to the scale of the features. By dividing by the standard deviation, we ensure that all features have a similar variance.\n",
    "\n",
    "2. Covariance matrix\n",
    "   \n",
    "   ```python\n",
    "   cov_mat = np.cov(X_std.T)\n",
    "   # Also can be replaced with this line\n",
    "   X_std.T@X_std/len(X_std)\n",
    "   ```\n",
    "   \n",
    "   Recall   \n",
    "  $$\n",
    "  Cov(X,Y) = \\mathbb{E}[(X-\\mathbb{E}(X))(Y-\\mathbb{E}(Y))]\n",
    "  = \\frac{1}{n-1} \\sum_{i=1}^N (x_i-\\bar{x})(y_i-\\bar{y})\n",
    "  $$\n",
    "\n",
    "  So we have $Cov(f_{d_1},f_{d_2}) = \\frac{1}{N} \\sum_{i=1}^N f_{id_1}f_{id_2}$ because the mean is 0.\n",
    "  \n",
    "  Here, why do we use `X_std.T@X_std/len(X_std)`\n",
    "\n",
    "  Recall the matrix $X$ with shape $N\\times d$\n",
    "  $$\n",
    "  \\left[\n",
    "  \\begin{matrix}\n",
    "  f_{11}&f_{12}&\\dots &f_{1d}\\\\\n",
    "  &\\vdots&\\\\\n",
    "  f_{N1}&f_{N2}&\\dots &f_{Nd}\\\\\n",
    "  \\end{matrix}\n",
    "  \\right]\n",
    "  $$\n",
    "\n",
    "  $X^T$ is this, with shape $d \\times N$:\n",
    "  $$\n",
    "  \\left[\n",
    "  \\begin{matrix}\n",
    "  f_{11}&f_{21}&\\dots &f_{N1}\\\\\n",
    "  &\\vdots&\\\\\n",
    "  f_{1d}&f_{2d}&\\dots &f_{Nd}\\\\\n",
    "  \\end{matrix}\n",
    "  \\right]\n",
    "  $$\n",
    "\n",
    "  So, $\\frac{1}{N}X^TX$ equals:\n",
    "  $$\n",
    "  \\left[\n",
    "  \\begin{matrix}\n",
    "  \\frac{1}{N}\\sum_{k=1}^N f_{k1}f_{k1} & \\frac{1}{N}\\sum_{k=1}^N f_{k1}f_{k2} & \\dots & \\frac{1}{N}\\sum_{k=1}^N f_{k1}f_{kd}\\\\\n",
    "  \\frac{1}{N}\\sum_{k=1}^N f_{k2}f_{k1} & \\frac{1}{N}\\sum_{k=1}^N f_{k2}f_{k2} & \\dots & \\frac{1}{N}\\sum_{k=1}^N f_{k2}f_{kd}\\\\\n",
    "  \\vdots&\\\\\n",
    "  \\frac{1}{N}\\sum_{k=1}^N f_{kd}f_{k1} & \\frac{1}{N}\\sum_{k=1}^N f_{kd}f_{k2} & \\dots & \\frac{1}{N}\\sum_{k=1}^N f_{kd}f_{kd}\\\\\n",
    "  \\end{matrix}\n",
    "  \\right]\n",
    "  $$\n",
    "\n",
    "  See what? the entry of the i-th row and j-th col is the covariance of feature i and feature j, that is why we need to normalize the data in the first step. And this matrix is symmetric.\n",
    "\n",
    "3. get the eigenvalues eigenvectors of the cov mat\n",
    "   \n",
    "   `We wish to find the direction that can maximize the variance, and the covariance of every pair of features is zero.`\n",
    "\n",
    "   So we need to diagonalize the covariance matrix, assume $P$ is the transformation matrix and assume $C = \\frac{1}{N}X^TX$\n",
    "\n",
    "   the covariance of $PX$ is:\n",
    "   $$\n",
    "   \\begin{aligned}\n",
    "   Cov(PX) &= \\frac{1}{N} PX(PX)^T \\\\\n",
    "   &=\\frac{1}{N} PXX^TP^T\\\\\n",
    "   &=P\\frac{1}{N} XX^TP^T\\\\\n",
    "   &=P(\\frac{1}{N} XX^T)P^T\\\\\n",
    "   \\end{aligned}\n",
    "   $$\n",
    "  \n",
    "   \n",
    "   $C$ is symmetric, we can find $N$ orthogonal eigenvectors $E=[e_1,\\dots,e_N]$, and we have\n",
    "   $$\n",
    "   E^TCE = \\Lambda = \\left(\n",
    "    \\begin{matrix}\n",
    "    \\lambda_1 &&&\\\\\n",
    "    &\\lambda_2&&\\\\\n",
    "    &&\\ddots&\\\\\n",
    "    &&&\\lambda_N\\\\\n",
    "    \\end{matrix}\n",
    "   \\right)\n",
    "   $$\n",
    "\n",
    "  where $\\Lambda$ is diagonal matrix, and $\\lambda_i$ is the eigenvectors.\n",
    "\n",
    "4. sort the eigenvalues in decreasing order and select top-k eigenvectors according to the eigenvalues\n",
    "   Here we just sort the eigenvectors and find top-k features with largest variance.\n",
    "   And we have already found the transformation matrix $P$ , $P= E^T$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
